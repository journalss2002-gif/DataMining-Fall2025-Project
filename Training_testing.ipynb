{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb8f250",
   "metadata": {},
   "source": [
    "Write Python code that loads a cleaned dataset, separates the features and target column ‘fraud’, performs a stratified train-test split (20% test size), and prints the shapes and fraud distribution in both training and testing sets. Keep it simple and readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a02b896b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (621868, 27) (621868,)\n",
      "Testing shape: (155468, 27) (155468,)\n",
      "\n",
      "Training fraud distribution:\n",
      "fraud\n",
      "0    99.825043\n",
      "1     0.174957\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Testing fraud distribution:\n",
      "fraud\n",
      "0    99.825044\n",
      "1     0.174956\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 1. LOAD CLEANED DATASET\n",
    "df_labeled = pd.read_csv(\"cleaned_dataset.csv\")\n",
    "\n",
    "\n",
    "# 2. SEPARATE FEATURES (X) AND TARGET (y)\n",
    "# Drop the target label \"fraud\" to get only the input features.\n",
    "# `y` holds the binary fraud indicator (0 = legitimate, 1 = fraud).\n",
    "X = df_labeled.drop(columns=[\"fraud\"])\n",
    "y = df_labeled[\"fraud\"]\n",
    "\n",
    "# 3. TRAIN–TEST SPLIT WITH STRATIFICATION\n",
    "\n",
    "# Splitting dataset into training (80%) and testing (20%).\n",
    "# `stratify=y` is CRITICAL for fraud datasets — ensures the fraud % stays\n",
    "# consistent across train and test sets despite imbalance.\n",
    "# `random_state=42` ensures reproducibility of the split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,          # 20% test size for evaluation\n",
    "    random_state=42,        # reproducible results\n",
    "    stratify=y              # preserves fraud ratio!\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 4. INSPECT SPLIT SHAPES + DISTRIBUTION\n",
    "\n",
    "# Confirms correct partition sizes and checks that stratification worked.\n",
    "print(\"Training shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"\\nTraining fraud distribution:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)   # % of fraud vs non-fraud\n",
    "\n",
    "print(\"\\nTesting fraud distribution:\")\n",
    "print(y_test.value_counts(normalize=True) * 100)    # should match training %\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "561b4a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\journ\\onedrive\\desktop\\cosc463_project\\venv\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in c:\\users\\journ\\onedrive\\desktop\\cosc463_project\\venv\\lib\\site-packages (from imbalanced-learn) (2.3.5)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in c:\\users\\journ\\onedrive\\desktop\\cosc463_project\\venv\\lib\\site-packages (from imbalanced-learn) (1.16.3)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in c:\\users\\journ\\onedrive\\desktop\\cosc463_project\\venv\\lib\\site-packages (from imbalanced-learn) (1.7.2)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in c:\\users\\journ\\onedrive\\desktop\\cosc463_project\\venv\\lib\\site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\journ\\onedrive\\desktop\\cosc463_project\\venv\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e63cb4",
   "metadata": {},
   "source": [
    "Write Python code that applies SMOTE to handle class imbalance in the fraud dataset.Use a sampling strategy of 0.1, apply it only on the training set to avoid data leakage, and print the class distribution before and after SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "101ac47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before SMOTE: fraud\n",
      "0    620780\n",
      "1      1088\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After SMOTE: fraud\n",
      "0    620780\n",
      "1     62078\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# APPLY SMOTE TO FIX EXTREME IMBALANCE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# SMOTE generates synthetic minority-class samples (fraud cases)\n",
    "# This helps the model learn fraud patterns instead of being overwhelmed by majority class.\n",
    "# sampling_strategy=0.1 → minority class becomes 10% of the training data\n",
    "# This is intentionally NOT 50/50 to avoid unrealistic inflation + overfitting.\n",
    "sm = SMOTE(sampling_strategy=0.1, random_state=42)\n",
    "\n",
    "# IMPORTANT:\n",
    "# Fit SMOTE **ONLY on the training set**.\n",
    "# Applying it to the test set would leak synthetic patterns → invalid evaluation!\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Compare original vs resampled class distribution\n",
    "print(\"\\nBefore SMOTE:\", y_train.value_counts())\n",
    "print(\"\\nAfter SMOTE:\", y_train_res.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66b642",
   "metadata": {},
   "source": [
    "Write Python code that trains a Logistic Regression and a Decision Tree for fraud detection.\n",
    "Both models should use class_weight='balanced' to handle imbalance, and they should be trained on the SMOTE-resampled data.\n",
    "Add clear comments explaining why class weighting matters and why each model is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cef67a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Complete: Logistic Regression + Decision Tree\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 1) LOGISTIC REGRESSION (WITH CLASS WEIGHTING)\n",
    "# class_weight=\"balanced\" forces LR to scale the importance of fraud cases.\n",
    "# Without this, LR would ignore minority class completely due to extreme imbalance.\n",
    "log_reg = LogisticRegression(\n",
    "    class_weight=\"balanced\",   # adjust weights inversely proportional to class frequency\n",
    "    max_iter=500,              # increases iterations so model fully converges\n",
    "    n_jobs=-1                  # use all CPU cores for faster training\n",
    ")\n",
    "\n",
    "# 2) DECISION TREE CLASSIFIER\n",
    "# Decision Trees capture nonlinear fraud patterns (e.g., amount thresholds, MCC behavior).\n",
    "# class_weight=\"balanced\" again prevents bias toward the majority class.\n",
    "tree_clf = DecisionTreeClassifier(\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42            \n",
    ")\n",
    "\n",
    "\n",
    "# TRAIN BOTH MODELS ON SMOTE-RESAMPLED DATA\n",
    "# We use X_train_res / y_train_res to ensure both models learn from a balanced structure.\n",
    "# SMOTE helps both linear (LR) and nonlinear (Tree) models better identify fraud signals.\n",
    "log_reg.fit(X_train_res, y_train_res)\n",
    "tree_clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "print(\"Model Training Complete: Logistic Regression + Decision Tree\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f10cb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated predictions on the true test set.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PREDICT ON ORIGINAL TEST SET (NO SMOTE HERE)\n",
    "\n",
    "# Logistic Regression predictions\n",
    "y_prob_log = log_reg.predict_proba(X_test)[:, 1]\n",
    "y_pred_log = (y_prob_log >= 0.5).astype(int)\n",
    "\n",
    "# Decision Tree predictions\n",
    "y_prob_tree = tree_clf.predict_proba(X_test)[:, 1]\n",
    "y_pred_tree = (y_prob_tree >= 0.5).astype(int)\n",
    "\n",
    "print(\"Generated predictions on the true test set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e59ca07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Logistic Regression Cross-Validation Results -----\n",
      "Precision (mean ± std): 0.1342 ± 0.0025\n",
      "Recall (mean ± std): 0.6363 ± 0.0052\n",
      "F1 (mean ± std): 0.2217 ± 0.0036\n",
      "Roc_auc (mean ± std): 0.6650 ± 0.0042\n",
      "\n",
      "----- Decision Tree Cross-Validation Results -----\n",
      "Precision (mean ± std): 0.9722 ± 0.0020\n",
      "Recall (mean ± std): 0.9698 ± 0.0019\n",
      "F1 (mean ± std): 0.9710 ± 0.0013\n",
      "Roc_auc (mean ± std): 0.9835 ± 0.0010\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#CROSS-VALIDATION FOR BOTH MODELS\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "# StratifiedKFold ensures each fold keeps the same fraud ratio.\n",
    "# This is CRITICAL for fraud modeling—regular KFold would randomly break the imbalance pattern.\n",
    "cv = StratifiedKFold(\n",
    "    n_splits=5,         # 5-fold CV = standard, stable\n",
    "    shuffle=True,       # shuffle ensures better randomness across folds\n",
    "    random_state=42     # reproducibility\n",
    ")\n",
    "\n",
    "# Metrics to evaluate across all folds.\n",
    "# We focus on precision, recall, F1, and ROC-AUC — the most important in fraud detection.\n",
    "scoring = {\n",
    "    \"precision\": \"precision\",\n",
    "    \"recall\": \"recall\",\n",
    "    \"f1\": \"f1\",\n",
    "    \"roc_auc\": \"roc_auc\"\n",
    "}\n",
    "\n",
    "# CROSS-VALIDATION — LOGISTIC REGRESSION\n",
    "\n",
    "log_cv = cross_validate(\n",
    "    log_reg,\n",
    "    X_train_res,        # SMOTE-resampled training data\n",
    "    y_train_res,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1           # use all CPU cores\n",
    ")\n",
    "\n",
    "\n",
    "# CROSS-VALIDATION — DECISION TREE\n",
    "\n",
    "tree_cv = cross_validate(\n",
    "    tree_clf,\n",
    "    X_train_res,\n",
    "    y_train_res,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "# DISPLAY CROSS-VALIDATION RESULTS \n",
    "\n",
    "def show_cv_results(name, cv_results):\n",
    "    print(f\"\\n----- {name} Cross-Validation Results -----\")\n",
    "    # Loop through each metric and display mean ± standard deviation across folds.\n",
    "    for metric in scoring.keys():\n",
    "        scores = cv_results[f\"test_{metric}\"]\n",
    "        print(f\"{metric.capitalize()} (mean ± std): {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "show_cv_results(\"Logistic Regression\", log_cv)\n",
    "show_cv_results(\"Decision Tree\", tree_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd45e89",
   "metadata": {},
   "source": [
    "Write Python code that performs 5-fold stratified cross-validation on both my Logistic Regression and Decision Tree models (trained on SMOTE-resampled data).\n",
    "Evaluate precision, recall, F1, and ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b81547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_recall_curve, roc_auc_score, auc,\n",
    "    precision_score, recall_score, f1_score, accuracy_score\n",
    ")\n",
    "\n",
    "def evaluate_model(name, y_test, y_pred, y_prob):\n",
    "    print(f\"\\n================ {name} =================\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
    "\n",
    "    # Core numeric metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    roc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    # PR-AUC\n",
    "    pr_precision, pr_recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(pr_recall, pr_precision)\n",
    "\n",
    "    # Create a DataFrame for a clean table display\n",
    "    results_table = pd.DataFrame({\n",
    "        \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\", \"PR-AUC\"],\n",
    "        \"Value\": [accuracy, precision, recall, f1, roc, pr_auc]\n",
    "    })\n",
    "\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(results_table.to_string(index=False))\n",
    "\n",
    "    # Optional full classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
